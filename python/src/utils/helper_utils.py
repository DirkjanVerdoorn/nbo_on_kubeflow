import os
import yaml
import pandas as pd
from typing import List, Text, Any, Optional

from tensorflow.python.lib.io import file_io
import tensorflow_transform as tft

from google.cloud import storage
from google.api_core import page_iterator

# read configurations from config file
def get_config(filename=None):
    """
    Creates configuration variables for model setup
    Args:
        filename    - name/path of yaml file 
                    (defaults to None)
    Returns:
        dictionary  - containing model configurations
    """
    if filename:
        yaml_file = filename
    else:
        yaml_file = 'config.yaml'

    with open(yaml_file, 'r') as stream:
        try:
            config = yaml.safe_load(stream)
        except yaml.YAMLError as exc:
            print(exc)
    return config

def get_vocabulary(vocab_root: Text, 
                column: Optional[Any]=None,
                combined_cols: Optional[bool]=False,
                **kwargs):
    """
    Description
    -----------
    Generates vocabulary list based on input file provided.

    Parameters
    ----------
    vocab_root (string): String indicating location in GCS where 
                            vocabulary csv file is stored.
    column (list): List containing column name(s) of dataframe 
                            needed to generate vocabulary
    cobmined_cols (bool): boolean indicating if vocabulary is to be created by
                            combining multiple columns in the input csv

    Returns
    -------
    Returns both the full vocabulary as well as the length of the vocabulary
    
    """

    if not combined_cols and type(column) == list:
        raise ValueError("""If combined_cols is set to False, 
        columns parameter should be single column name of type string""")

    with file_io.FileIO(vocab_root, 'r') as f:
        vocab = pd.read_csv(f)

    if combined_cols:
        vocab['joined'] = vocab[column].agg('-'.join, axis=1)
        vocabulary = vocab['joined'].tolist()
    else:
        vocabulary = vocab[column].tolist()

    return vocabulary, len(vocabulary)

def get_tft_vocab(
                name: Text, 
                bucket: Text, 
                directory: Text,
                transform_output: Text,
                full_vocab: Optional[bool]=False):
    """
    Description
    -----------
    Reads vocabulary generated by the Transform component
    using the tft.compute_and_apply_vocabulary function. 

    Parameters
    -----------
    name (str): name of vocabulary
    bucket (str): name of bucket that contains transform output
    directory (str): directory to transform output regarding vocabulary
    transform_output (str): location of transform comnponent output

    Returns
    -------
    int indicating size of vocabulary
    """

    def _item_to_value(iterator, item):
        return item

    def _list_sub_directories(bucket: Text, prefix: Text):
        if prefix and not prefix.endswith('/'):
            prefix += '/'

        extra_params = {
            'projection': 'noAcl',
            'prefix': prefix,
            'delimiter': '/'
        }

        gcs = storage.Client()

        path = '/b/' + bucket + '/o'

        directories = page_iterator.HTTPIterator(
            client=gcs,
            api_request=gcs._connection.api_request,
            path=path,
            items_key='prefixes',
            item_to_value=_item_to_value,
            extra_params=extra_params
        )

        return str(max([int(i.rsplit('/', 2)[1]) for i in directories]))

    recent_dir = _list_sub_directories(bucket=bucket, prefix=directory)
    recent_transform_output = os.path.join(transform_output, recent_dir)
    
    tf_transform_output = tft.TFTransformOutput(recent_transform_output)
    
    if not full_vocab:
        return tf_transform_output.vocabulary_size_by_name(name)
    elif full_vocab:
        vocab = tf_transform_output.vocabulary_by_name(name)
        vocab = [i.decode('utf-8') for i in vocab]
        return vocab

def _transformed_name(x):
    return x + '_xf'


def get_latest_version(bucket: Text, prefix: Text):
    """
    Description
    -----------

    Parameters
    ----------

    Returns
    -------
    """

    def _item_to_value(iterator, item):
        return item

    if prefix and not prefix.endswith('/'):
        prefix += '/'
    
    extra_params = {
        'projection': 'noAcl',
        'prefix': prefix,
        'delimiter': '/'
    }

    gcs = storage.Client()
    path = '/b/' + bucket + '/o'

    directories = page_iterator.HTTPIterator(
        client=gcs,
        api_request=gcs._connection.api_request,
        path=path,
        items_key='prefixes',
        item_to_value=_item_to_value,
        extra_params=extra_params
    )
    
    return str(max([int(i.rsplit('/', 2)[1]) for i in directories]))

